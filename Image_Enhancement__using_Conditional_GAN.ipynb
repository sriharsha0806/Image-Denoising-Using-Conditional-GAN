{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Enhancement _using Conditional_GAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMpDZAyLCaBx",
        "colab_type": "text"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOkF16FRmuC2",
        "colab_type": "text"
      },
      "source": [
        "Given the original and degraded versions of a few images. The task is to write a GAN which can fix the degraded images.\n",
        "\n",
        "Completed the function `fix` at the end of the \"Evaluation\" block so that it can take a degraded image, and return a fixed image (that looks as much like the original non-degraded version as possible). Read the doc-string of the fix function to see the format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insnPyAeRC0x",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoYMgagKCFdn",
        "colab_type": "text"
      },
      "source": [
        "## Intended Structure after Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCHJeTjnhj05",
        "colab_type": "text"
      },
      "source": [
        "Run the blocks in this section to get the following directory structure:\n",
        "```\n",
        "/content\n",
        "│\n",
        "└───rephrase-pubfig831\n",
        "    │\n",
        "    └───correct\n",
        "    │   │\n",
        "    │   └───train\n",
        "    │   │   │\n",
        "    │   │   └───Adam Sandler\n",
        "    │   │   │   │   train__000001-000000.jpg\n",
        "    │   │   │   │   train__000001-000001.jpg\n",
        "    │   │   │   │   train__000001-000002.jpg\n",
        "    │   │   │   │   ...\n",
        "    │   │   │\n",
        "    │   │   └───Alec Baldwin\n",
        "    │   │   │   │   train__000002-000000.jpg\n",
        "    │   │   │   │   train__000002-000001.jpg\n",
        "    │   │   │   │   ...\n",
        "    │   │   │\n",
        "    │   │   └───Angelina Jolie\n",
        "    │   │   │   │   train__000003-000000.jpg\n",
        "    │   │   │   │   train__000003-000001.jpg\n",
        "    │   │   │   │   ...\n",
        "    │   │   │\n",
        "    │   │   │ ...\n",
        "    │   │\n",
        "    │   └───test\n",
        "    │       │\n",
        "    │       └───Adam Sandler\n",
        "    │       │   │   test__000001-000000.jpg\n",
        "    │       │   │   test__000001-000001.jpg\n",
        "    │       │   │   ...\n",
        "    │       │\n",
        "    │       └───Alec Baldwin\n",
        "    │       │   │   test__000002-000000.jpg\n",
        "    │       │   │   ...\n",
        "    │       │\n",
        "    │       └───Angelina Jolie\n",
        "    │       │   │   test__000003-000000.jpg\n",
        "    │       │   │   ...\n",
        "    │       │\n",
        "    │       │ ...\n",
        "    │\n",
        "    │\n",
        "    └───degraded\n",
        "        │   <Same directory structure as 'correct'>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58tM3kFPNZ0Z",
        "colab_type": "text"
      },
      "source": [
        "Every image in the degraded directory is a degraded version of the image with the same name in the correct directory. e.g. `/content/rephrase-pubfig831/degraded/Adam Sandler/train__000001-000002.jpg` is the degraded version of `/content/rephrase-pubfig831/correct/Adam Sandler/train__000001-000002.jpg`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrGJB04RF4d",
        "colab_type": "text"
      },
      "source": [
        "## Installation (pip etc)\n",
        "Add any other installation commands you want to in this block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ljp1DHRNt7",
        "colab_type": "code",
        "outputId": "16abb1e4-8941-49a8-ba4e-604166e4de47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install GPUtil\n",
        "!pip install tqdm\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=acefa72e253209be8524ad04f797eb9342c56ff218e53f92c71a38aa9234dd81\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNJbKsnjR74d",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Generating Dataset\n",
        "Run this block only once. Do not modify it. Also, don't call the degrade function in your code anywhere. You should treat the degradation process as a black box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozoy8Olklwaj",
        "colab_type": "code",
        "outputId": "49281344-32ba-43b8-c189-e4cc339d994b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def degrade(path: str) -> None:\n",
        "    \"\"\"Load image at `input_path`, distort and save as `output_path`\"\"\"\n",
        "    SHIFT = 2\n",
        "    image = cv2.imread(path)\n",
        "    to_swap = np.random.choice([False, True], image.shape[:2], p=[.8, .2])\n",
        "    swap_indices = np.where(to_swap[:-SHIFT] & ~to_swap[SHIFT:])\n",
        "    swap_vals = image[swap_indices[0] + SHIFT, swap_indices[1]]\n",
        "    image[swap_indices[0] + SHIFT, swap_indices[1]] = image[swap_indices]\n",
        "    image[swap_indices] = swap_vals\n",
        "    cv2.imwrite(path, image)\n",
        "\n",
        "!wget http://briancbecker.com/files/downloads/pubfig83lfw/pubfig83lfw_raw_in_dirs.zip\n",
        "!unzip -q pubfig83lfw_raw_in_dirs.zip\n",
        "!rm pubfig83lfw_raw_in_dirs.zip\n",
        "!mkdir rephrase-pubfig831\n",
        "!mv pubfig83lfw_raw_in_dirs rephrase-pubfig831/correct\n",
        "!rm -r rephrase-pubfig831/correct/distract\n",
        "!cp -r rephrase-pubfig831/correct rephrase-pubfig831/degraded\n",
        "\n",
        "for image_path in tqdm(glob('rephrase-pubfig831/degraded/*/*/*.jpg')):\n",
        "  degrade(image_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-25 14:38:01--  http://briancbecker.com/files/downloads/pubfig83lfw/pubfig83lfw_raw_in_dirs.zip\n",
            "Resolving briancbecker.com (briancbecker.com)... 162.241.216.158\n",
            "Connecting to briancbecker.com (briancbecker.com)|162.241.216.158|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400247922 (382M) [application/zip]\n",
            "Saving to: ‘pubfig83lfw_raw_in_dirs.zip’\n",
            "\n",
            "pubfig83lfw_raw_in_ 100%[===================>] 381.71M  97.4MB/s    in 4.1s    \n",
            "\n",
            "2019-11-25 14:38:05 (92.5 MB/s) - ‘pubfig83lfw_raw_in_dirs.zip’ saved [400247922/400247922]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 5130/13002 [00:24<00:36, 218.41it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz5BM22VralE",
        "colab_type": "text"
      },
      "source": [
        "# **Checking Free Memory**\n",
        "This block is just so that you can have an idea of the resources you have at hand on the Google Collab system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoMS9HMX6G9D",
        "colab_type": "code",
        "outputId": "d10c00dc-1ca6-4458-8963-cad190db65f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "gpu = GPU.getGPUs()[0]\n",
        "process = psutil.Process(os.getpid())\n",
        "print(f\"Gen RAM: Free {humanize.naturalsize(psutil.virtual_memory().available)} | Proc size {humanize.naturalsize(process.memory_info().rss)}\")\n",
        "print(f\"GPU RAM: Free {gpu.memoryFree:.0f}MB | Used {gpu.memoryUsed:.0f}MB | Util {gpu.memoryUtil*100:.0f}% | Total {gpu.memoryTotal:.0f}MB\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM: Free 12.8 GB | Proc size 177.0 MB\n",
            "GPU RAM: Free 16280MB | Used 0MB | Util 0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMubMpKy15ac",
        "colab_type": "text"
      },
      "source": [
        "# **Main Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIuewxbgsM4d",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4DoUU8GYJes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import random\n",
        "from itertools import chain\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "def prepend(list, str):       \n",
        "    # Using format() \n",
        "    str += '{0}'\n",
        "    list = [str.format(i) for i in list] \n",
        "    return(list)\n",
        "    \n",
        "class DatasetFromFolder(data.Dataset):\n",
        "    def __init__(self, image_dir, subpath = ['correct/', 'degraded/'], subfolder='train', direction='AtoB'):\n",
        "        super(DatasetFromFolder, self).__init__()\n",
        "        \n",
        "        self.input_path_a = os.path.join(image_dir, subpath[0], subfolder)\n",
        "        self.classes_a = [os.path.join(self.input_path_a,x) for x in sorted(os.listdir(self.input_path_a))]\n",
        "        self.image_filenames_a = []\n",
        "        for class_ in self.classes_a:\n",
        "            self.image_filenames_a.append(prepend(sorted(os.listdir(class_)), class_ + '/'))\n",
        "        self.image_filenames_a = list(chain.from_iterable(self.image_filenames_a))\n",
        "        \n",
        "        self.image_filenames_b = []\n",
        "        for files in self.image_filenames_a:\n",
        "            files = files.split('/')\n",
        "            files[1] = subpath[1][:-1]\n",
        "            files = '/'.join(files)\n",
        "            self.image_filenames_b.append(files)\n",
        "        assert len(self.image_filenames_b) == len(self.image_filenames_a), \"number of files are differemt\"\n",
        "        self.direction = direction\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Load Image        \n",
        "        a = Image.open(self.image_filenames_a[index])\n",
        "        b = Image.open(self.image_filenames_b[index])   \n",
        "        \n",
        "        a = a.resize((286, 286), Image.BICUBIC)\n",
        "        b = b.resize((286, 286), Image.BICUBIC)\n",
        "        a = transforms.ToTensor()(a)\n",
        "        b = transforms.ToTensor()(b)\n",
        "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "    \n",
        "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "    \n",
        "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
        "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
        "            idx = torch.LongTensor(idx)\n",
        "            a = a.index_select(2, idx)\n",
        "            b = b.index_select(2, idx)\n",
        "\n",
        "        if self.direction == \"AtoB\":\n",
        "            return a, b\n",
        "        else:\n",
        "            return b, a\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSV-9eYyfbX2",
        "colab_type": "text"
      },
      "source": [
        "## Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqLOT8BUsAaW",
        "colab_type": "text"
      },
      "source": [
        "### **Constants and Hyperparemeters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8NNBxqO4qPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'rephrase-pubfig831'\n",
        "subpath = ['correct/', 'degraded/']\n",
        "subfolder='train'\n",
        "direction = \"BtoA\"\n",
        "filters_gen = 16\n",
        "filters_dis = 16\n",
        "input_size = 256\n",
        "resize = 286\n",
        "crop_size = 256\n",
        "fliplr = True\n",
        "num_epochs = 40\n",
        "DISCRIMINATOR_FINAL_FEATURE_MAP_SIZE = 10\n",
        "RESIUDAL_BLOCKS = 16\n",
        "UPSAMPLING_BLOCKS = 2\n",
        "lr = 0.001\n",
        "batch_size = 1\n",
        "lr_b1 = 0.9\n",
        "lr_b2 = 0.999\n",
        "\n",
        "\n",
        "\n",
        "# Directory for saving the model\n",
        "model_dir = dataset + '/' + '_model/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQr7M263s-CE",
        "colab_type": "text"
      },
      "source": [
        "### Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLjLc0INPJY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        vgg19_model = vgg19(pretrained=True)\n",
        "        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.vgg19_54(img)\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n",
        "            if non_linearity:\n",
        "                layers += [nn.LeakyReLU()]\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class GeneratorRRDB(nn.Module):\n",
        "    def __init__(self, channels, filters=16, num_res_blocks=4, num_upsample=2):\n",
        "        super(GeneratorRRDB, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n",
        "        # Residual blocks\n",
        "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "        # Upsampling layers\n",
        "        upsample_layers = []\n",
        "        for _ in range(num_upsample):\n",
        "            upsample_layers += [\n",
        "                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=2, padding=1),\n",
        "                nn.LeakyReLU(),\n",
        "                nn.PixelShuffle(upscale_factor=2),\n",
        "            ]\n",
        "        self.upsampling = nn.Sequential(*upsample_layers)\n",
        "        # Final output block\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u7Eq3yltYld",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6tsJqgi-Gd",
        "colab_type": "code",
        "outputId": "4fa28f36-7bd1-4b39-9a69-1e301743441b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def sigmoid_mul(x):\n",
        "    return x * F.sigmoid(x)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, cnn, feature_layer=11):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.features = nn.Sequential(*list(cnn.features.children())[:(feature_layer+1)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "\n",
        "class residualBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, k=3, n=64, s=1):\n",
        "        super(residualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, n, k, stride=s, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n)\n",
        "        self.conv2 = nn.Conv2d(n, n, k, stride=s, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = sigmoid_mul(self.bn1(self.conv1(x)))\n",
        "        return self.bn2(self.conv2(y)) + x\n",
        "\n",
        "class upsampleBlock(nn.Module):\n",
        "    # Implements resize-convolution\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(upsampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
        "        self.shuffler = nn.PixelShuffle(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return sigmoid_mul(self.shuffler(self.conv(x)))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_residual_blocks, upsample_factor):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_residual_blocks = n_residual_blocks\n",
        "        self.upsample_factor = upsample_factor\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 9, stride=1, padding=4)\n",
        "\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            self.add_module('residual_block' + str(i+1), residualBlock())\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        for i in range(int(self.upsample_factor/2)):\n",
        "            self.add_module('upsample' + str(i+1), upsampleBlock(64, 256))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 3, 9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = sigmoid_mul(self.conv1(x))\n",
        "\n",
        "        y = x.clone()\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            y = self.__getattr__('residual_block' + str(i+1))(y)\n",
        "\n",
        "        x = self.bn2(self.conv2(y)) + x\n",
        "\n",
        "        for i in range(int(self.upsample_factor/2)):\n",
        "            x = self.__getattr__('upsample' + str(i+1))(x)\n",
        "\n",
        "        return self.conv3(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Replaced original paper FC layers with FCN\n",
        "        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = sigmoid_mul(self.conv1(x))\n",
        "\n",
        "        x = sigmoid_mul(self.bn2(self.conv2(x)))\n",
        "        x = sigmoid_mul(self.bn3(self.conv3(x)))\n",
        "        x = sigmoid_mul(self.bn4(self.conv4(x)))\n",
        "        x = sigmoid_mul(self.bn5(self.conv5(x)))\n",
        "        x = sigmoid_mul(self.bn6(self.conv6(x)))\n",
        "        x = sigmoid_mul(self.bn7(self.conv7(x)))\n",
        "        x = sigmoid_mul(self.bn8(self.conv8(x)))\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\ndef sigmoid_mul(x):\\n    return x * F.sigmoid(x)\\n\\nclass FeatureExtractor(nn.Module):\\n    def __init__(self, cnn, feature_layer=11):\\n        super(FeatureExtractor, self).__init__()\\n        self.features = nn.Sequential(*list(cnn.features.children())[:(feature_layer+1)])\\n\\n    def forward(self, x):\\n        return self.features(x)\\n\\n\\nclass residualBlock(nn.Module):\\n    def __init__(self, in_channels=64, k=3, n=64, s=1):\\n        super(residualBlock, self).__init__()\\n\\n        self.conv1 = nn.Conv2d(in_channels, n, k, stride=s, padding=1)\\n        self.bn1 = nn.BatchNorm2d(n)\\n        self.conv2 = nn.Conv2d(n, n, k, stride=s, padding=1)\\n        self.bn2 = nn.BatchNorm2d(n)\\n\\n    def forward(self, x):\\n        y = sigmoid_mul(self.bn1(self.conv1(x)))\\n        return self.bn2(self.conv2(y)) + x\\n\\nclass upsampleBlock(nn.Module):\\n    # Implements resize-convolution\\n    def __init__(self, in_channels, out_channels):\\n        super(upsampleBlock, self).__init__()\\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\\n        self.shuffler = nn.PixelShuffle(2)\\n\\n    def forward(self, x):\\n        return sigmoid_mul(self.shuffler(self.conv(x)))\\n\\nclass Generator(nn.Module):\\n    def __init__(self, n_residual_blocks, upsample_factor):\\n        super(Generator, self).__init__()\\n        self.n_residual_blocks = n_residual_blocks\\n        self.upsample_factor = upsample_factor\\n\\n        self.conv1 = nn.Conv2d(3, 64, 9, stride=1, padding=4)\\n\\n        for i in range(self.n_residual_blocks):\\n            self.add_module('residual_block' + str(i+1), residualBlock())\\n\\n        self.conv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\\n        self.bn2 = nn.BatchNorm2d(64)\\n\\n        for i in range(int(self.upsample_factor/2)):\\n            self.add_module('upsample' + str(i+1), upsampleBlock(64, 256))\\n\\n        self.conv3 = nn.Conv2d(64, 3, 9, stride=1, padding=4)\\n\\n    def forward(self, x):\\n        x = sigmoid_mul(self.conv1(x))\\n\\n        y = x.clone()\\n        for i in range(self.n_residual_blocks):\\n            y = self.__getattr__('residual_block' + str(i+1))(y)\\n\\n        x = self.bn2(self.conv2(y)) + x\\n\\n        for i in range(int(self.upsample_factor/2)):\\n            x = self.__getattr__('upsample' + str(i+1))(x)\\n\\n        return self.conv3(x)\\n\\n\\nclass Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\\n\\n        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\\n        self.bn2 = nn.BatchNorm2d(64)\\n        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\\n        self.bn3 = nn.BatchNorm2d(128)\\n        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\\n        self.bn4 = nn.BatchNorm2d(128)\\n        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\\n        self.bn5 = nn.BatchNorm2d(256)\\n        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\\n        self.bn6 = nn.BatchNorm2d(256)\\n        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\\n        self.bn7 = nn.BatchNorm2d(512)\\n        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\\n        self.bn8 = nn.BatchNorm2d(512)\\n\\n        # Replaced original paper FC layers with FCN\\n        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\\n\\n    def forward(self, x):\\n        x = sigmoid_mul(self.conv1(x))\\n\\n        x = sigmoid_mul(self.bn2(self.conv2(x)))\\n        x = sigmoid_mul(self.bn3(self.conv3(x)))\\n        x = sigmoid_mul(self.bn4(self.conv4(x)))\\n        x = sigmoid_mul(self.bn5(self.conv5(x)))\\n        x = sigmoid_mul(self.bn6(self.conv6(x)))\\n        x = sigmoid_mul(self.bn7(self.conv7(x)))\\n        x = sigmoid_mul(self.bn8(self.conv8(x)))\\n\\n        x = self.conv9(x)\\n        return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVjtttv0fKhp"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD0xLY_FfRRO",
        "colab_type": "code",
        "outputId": "46d52290-de79-466f-baeb-baea009baf73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torchvision\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#generator = Generator(16, 2)\n",
        "generator = GeneratorRRDB(3, filters=64, num_res_blocks=2).to(device)\n",
        "discriminator = Discriminator(input_shape=(3,256,256)).to(device)\n",
        "#discriminator = Discriminator()\n",
        "feature_extractor = FeatureExtractor().to(device)\n",
        "\n",
        "# set feature extractor to inference mode\n",
        "feature_extractor.eval()\n",
        "\n",
        "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)G = torch.load(model_dir + 'generator_param.pth')\n",
        "criterion_content = torch.nn.L1Loss().to(device)\n",
        "criterion_pixel = torch.nn.L1Loss().to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:08<00:00, 71.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OHWHhltZfP2r"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhL1Myvmopcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(lr_b1, lr_b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(lr_b1, lr_b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_eANXWf52U",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No1MjnCQsoP9",
        "colab_type": "text"
      },
      "source": [
        "### Setting device to use for tensor operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZ5YQ4uJbP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = DatasetFromFolder(dataset, direction=direction)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                                batch_size=1,\n",
        "                                                shuffle=True)\n",
        "test_data = DatasetFromFolder(dataset, subfolder=\"test\", direction=direction)\n",
        "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                                batch_size=1,\n",
        "                                                shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q_IX5GGs0hh",
        "colab_type": "text"
      },
      "source": [
        "### Initializing weights (if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huQiEB4sLP4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7msb0gPuC2A",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Ccj64AtlK8",
        "colab_type": "code",
        "outputId": "1a0fcd22-681c-4e2a-c59a-b9a734f7f9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i_,j_ = 0, 0\n",
        "from torch.autograd import Variable\n",
        "for epoch in range(40):\n",
        "    for i, (input_, target) in enumerate(train_data_loader):\n",
        "        imgs_lr, imgs_hr = Variable(input_.type(Tensor)), Variable(target.type(Tensor))\n",
        "        iterations = epoch*len(train_data_loader)+i \n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "        optimizer_G.zero_grad()\n",
        "        gen_hr = generator(imgs_lr)\n",
        "        #print(gen_hr.size())\n",
        "        loss_pixel = criterion_pixel(gen_hr, imgs_hr)\n",
        "        if iterations < 500:\n",
        "            i_ += 1\n",
        "\n",
        "            loss_pixel.backward()\n",
        "            optimizer_G.step()\n",
        "            if i_ > 800:\n",
        "                print(\n",
        "                    \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
        "                    % (epoch, 40, i, len(train_data_loader), loss_pixel.item())\n",
        "                )\n",
        "                i_ = 0\n",
        "            continue\n",
        "\n",
        "        pred_real = discriminator(imgs_hr).detach()\n",
        "        pred_fake = discriminator(gen_hr)\n",
        "\n",
        "        loss_GAN = criterion_GAN(pred_fake-pred_real.mean(0, keepdim=True), valid)\n",
        "\n",
        "        # content loss\n",
        "        gen_features = feature_extractor(gen_hr)\n",
        "        real_features = feature_extractor(imgs_hr).detach()\n",
        "        loss_content = criterion_content(gen_features, real_features)\n",
        "\n",
        "        loss_G = loss_content + 5e-3*loss_GAN + 1e-2*loss_pixel\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        pred_real = discriminator(imgs_hr)\n",
        "        pred_fake = discriminator(gen_hr.detach())\n",
        "\n",
        "        loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
        "        loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
        "\n",
        "        loss_D = (loss_real + loss_fake)/2\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "        j_ += 1\n",
        "        if j_ > 1000:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, content: %f, adv: %f, pixel: %f]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    40,\n",
        "                    i,\n",
        "                    len(train_data_loader),\n",
        "                    loss_D.item(),\n",
        "                    loss_G.item(),\n",
        "                    loss_content.item(),\n",
        "                    loss_GAN.item(),\n",
        "                    loss_pixel.item(),\n",
        "                )\n",
        "            )\n",
        "            j_ = 0\n",
        "    torch.save(generator, model_dir + 'generator_param.pth')\n",
        "    torch.save(discriminator, model_dir + 'discriminator_param.pth')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/40] [Batch 1500/8720] [D loss: 0.000055] [G loss: 2.451745, content: 2.361094, adv: 13.708691, pixel: 2.210711]\n",
            "[Epoch 0/40] [Batch 2501/8720] [D loss: 0.000019] [G loss: 1.970237, content: 1.876372, adv: 13.642138, pixel: 2.565462]\n",
            "[Epoch 0/40] [Batch 3502/8720] [D loss: 0.000014] [G loss: 2.366439, content: 2.254105, adv: 14.364664, pixel: 4.051015]\n",
            "[Epoch 0/40] [Batch 4503/8720] [D loss: 0.000003] [G loss: 2.045160, content: 1.943473, adv: 15.943725, pixel: 2.196787]\n",
            "[Epoch 0/40] [Batch 5504/8720] [D loss: 0.000012] [G loss: 2.369820, content: 2.229564, adv: 23.037378, pixel: 2.506909]\n",
            "[Epoch 0/40] [Batch 6505/8720] [D loss: 0.000008] [G loss: 2.430858, content: 2.293442, adv: 23.135441, pixel: 2.173901]\n",
            "[Epoch 0/40] [Batch 7506/8720] [D loss: 0.000003] [G loss: 2.817745, content: 2.680782, adv: 24.021860, pixel: 1.685330]\n",
            "[Epoch 0/40] [Batch 8507/8720] [D loss: 0.000016] [G loss: 2.021290, content: 1.854536, adv: 26.510788, pixel: 3.419985]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GeneratorRRDB. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResidualInResidualDenseBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DenseResidualBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LeakyReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PixelShuffle. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/40] [Batch 788/8720] [D loss: 0.000010] [G loss: 1.579724, content: 1.439387, adv: 22.835571, pixel: 2.615944]\n",
            "[Epoch 1/40] [Batch 1789/8720] [D loss: 0.000002] [G loss: 1.421168, content: 1.282766, adv: 24.660931, pixel: 1.509767]\n",
            "[Epoch 1/40] [Batch 2790/8720] [D loss: 0.000004] [G loss: 1.360244, content: 1.245099, adv: 20.789249, pixel: 1.119895]\n",
            "[Epoch 1/40] [Batch 3791/8720] [D loss: 0.000003] [G loss: 0.830386, content: 0.695942, adv: 25.081345, pixel: 0.903750]\n",
            "[Epoch 1/40] [Batch 4792/8720] [D loss: 0.001739] [G loss: 0.650007, content: 0.597509, adv: 10.049887, pixel: 0.224910]\n",
            "[Epoch 1/40] [Batch 5793/8720] [D loss: 0.002791] [G loss: 0.868588, content: 0.830222, adv: 7.285985, pixel: 0.193555]\n",
            "[Epoch 1/40] [Batch 6794/8720] [D loss: 0.001043] [G loss: 0.505066, content: 0.453003, adv: 10.055607, pixel: 0.178439]\n",
            "[Epoch 1/40] [Batch 7795/8720] [D loss: 0.000094] [G loss: 457956.375000, content: 450030.937500, adv: 11.806261, pixel: 792535.937500]\n",
            "[Epoch 2/40] [Batch 76/8720] [D loss: 0.000019] [G loss: 63745.437500, content: 62531.363281, adv: 15.937435, pixel: 121399.687500]\n",
            "[Epoch 2/40] [Batch 1077/8720] [D loss: 0.000005] [G loss: 21424.804688, content: 20465.542969, adv: 17.133984, pixel: 95917.625000]\n",
            "[Epoch 2/40] [Batch 2078/8720] [D loss: 0.001585] [G loss: 11995.019531, content: 11593.119141, adv: 14.741055, pixel: 40182.738281]\n",
            "[Epoch 2/40] [Batch 3079/8720] [D loss: 0.000001] [G loss: 5297.409180, content: 5069.332031, adv: 24.989174, pixel: 22795.218750]\n",
            "[Epoch 2/40] [Batch 4080/8720] [D loss: 0.000000] [G loss: 10483.273438, content: 10260.591797, adv: 28.015686, pixel: 22254.164062]\n",
            "[Epoch 2/40] [Batch 5081/8720] [D loss: 0.000000] [G loss: 8185.686523, content: 7936.754883, adv: 23.562176, pixel: 24881.406250]\n",
            "[Epoch 2/40] [Batch 6082/8720] [D loss: 0.000003] [G loss: 2768.805176, content: 2611.501465, adv: 28.608051, pixel: 15716.075195]\n",
            "[Epoch 2/40] [Batch 7083/8720] [D loss: 0.000001] [G loss: 840.386902, content: 785.878052, adv: 34.510735, pixel: 5433.630371]\n",
            "[Epoch 2/40] [Batch 8084/8720] [D loss: 0.000000] [G loss: 435.934570, content: 406.114624, adv: 33.094490, pixel: 2965.447021]\n",
            "[Epoch 3/40] [Batch 365/8720] [D loss: 0.000000] [G loss: 578.151611, content: 545.113892, adv: 36.069981, pixel: 3285.733643]\n",
            "[Epoch 3/40] [Batch 1366/8720] [D loss: 0.000021] [G loss: 1234789.750000, content: 1183926.250000, adv: 44.572231, pixel: 5086324.500000]\n",
            "[Epoch 3/40] [Batch 2367/8720] [D loss: 0.000000] [G loss: 596875.312500, content: 566084.437500, adv: 47.596443, pixel: 3079061.250000]\n",
            "[Epoch 3/40] [Batch 3368/8720] [D loss: 0.000000] [G loss: 326574.375000, content: 308721.875000, adv: 51.392006, pixel: 1785223.625000]\n",
            "[Epoch 3/40] [Batch 4369/8720] [D loss: 0.000000] [G loss: 250101.234375, content: 233113.343750, adv: 50.015823, pixel: 1698764.375000]\n",
            "[Epoch 3/40] [Batch 5370/8720] [D loss: 0.000000] [G loss: 260602.343750, content: 236617.828125, adv: 50.490913, pixel: 2398426.750000]\n",
            "[Epoch 3/40] [Batch 6371/8720] [D loss: 0.000000] [G loss: 115894.015625, content: 107948.890625, adv: 48.028217, pixel: 794488.312500]\n",
            "[Epoch 3/40] [Batch 7372/8720] [D loss: 0.000000] [G loss: 62476.441406, content: 55990.031250, adv: 47.647591, pixel: 648617.312500]\n",
            "[Epoch 3/40] [Batch 8373/8720] [D loss: 0.000000] [G loss: 4158966.000000, content: 3944216.750000, adv: 56.402660, pixel: 21474896.000000]\n",
            "[Epoch 4/40] [Batch 654/8720] [D loss: 0.000001] [G loss: 576689.875000, content: 525112.187500, adv: 48.886055, pixel: 5157742.500000]\n",
            "[Epoch 4/40] [Batch 1655/8720] [D loss: 0.000000] [G loss: 259396.578125, content: 235009.625000, adv: 56.735634, pixel: 2438667.000000]\n",
            "[Epoch 4/40] [Batch 2656/8720] [D loss: 0.000000] [G loss: 96993.789062, content: 89627.906250, adv: 53.687840, pixel: 736561.937500]\n",
            "[Epoch 4/40] [Batch 3657/8720] [D loss: 0.000000] [G loss: 75212.726562, content: 69345.265625, adv: 56.933586, pixel: 586718.000000]\n",
            "[Epoch 4/40] [Batch 4658/8720] [D loss: 0.000000] [G loss: 124358.406250, content: 119235.625000, adv: 57.511871, pixel: 512249.125000]\n",
            "[Epoch 4/40] [Batch 5659/8720] [D loss: 0.000000] [G loss: 23574.763672, content: 21789.656250, adv: 53.434856, pixel: 178483.937500]\n",
            "[Epoch 4/40] [Batch 6660/8720] [D loss: 0.000000] [G loss: 21359.744141, content: 20170.259766, adv: 57.606483, pixel: 118919.773438]\n",
            "[Epoch 4/40] [Batch 7661/8720] [D loss: 0.000003] [G loss: 9889952768.000000, content: 9726623744.000000, adv: 28.714787, pixel: 16332942336.000000]\n",
            "[Epoch 4/40] [Batch 8662/8720] [D loss: 0.000000] [G loss: 4072581120.000000, content: 3982698496.000000, adv: 47.929836, pixel: 8988267520.000000]\n",
            "[Epoch 5/40] [Batch 943/8720] [D loss: 0.000000] [G loss: 2064412672.000000, content: 1991447552.000000, adv: 42.672409, pixel: 7296506368.000000]\n",
            "[Epoch 5/40] [Batch 1944/8720] [D loss: 0.000000] [G loss: 902654592.000000, content: 862468736.000000, adv: 38.903717, pixel: 4018582784.000000]\n",
            "[Epoch 5/40] [Batch 2945/8720] [D loss: 0.000000] [G loss: 573352128.000000, content: 547588224.000000, adv: 38.206192, pixel: 2576390912.000000]\n",
            "[Epoch 5/40] [Batch 3946/8720] [D loss: 0.000000] [G loss: 1186090112.000000, content: 1076916480.000000, adv: 44.201141, pixel: 10917361664.000000]\n",
            "[Epoch 5/40] [Batch 4947/8720] [D loss: 0.000000] [G loss: 306953184.000000, content: 270456224.000000, adv: 45.157043, pixel: 3649694720.000000]\n",
            "[Epoch 5/40] [Batch 5948/8720] [D loss: 0.000000] [G loss: 1670784000.000000, content: 1626104064.000000, adv: 49.351593, pixel: 4467990528.000000]\n",
            "[Epoch 5/40] [Batch 6949/8720] [D loss: 0.000000] [G loss: 100234448.000000, content: 90196304.000000, adv: 48.910866, pixel: 1003814592.000000]\n",
            "[Epoch 5/40] [Batch 7950/8720] [D loss: 0.000000] [G loss: 70025400.000000, content: 65764696.000000, adv: 45.382915, pixel: 426070272.000000]\n",
            "[Epoch 6/40] [Batch 231/8720] [D loss: 0.000000] [G loss: 27517786.000000, content: 24973540.000000, adv: 48.243759, pixel: 254424688.000000]\n",
            "[Epoch 6/40] [Batch 1232/8720] [D loss: 0.000000] [G loss: 812790185984.000000, content: 779547705344.000000, adv: 45.232632, pixel: 3324245704704.000000]\n",
            "[Epoch 6/40] [Batch 2233/8720] [D loss: 0.000000] [G loss: 93863755776.000000, content: 86384599040.000000, adv: 49.236557, pixel: 747916034048.000000]\n",
            "[Epoch 6/40] [Batch 3234/8720] [D loss: 0.000000] [G loss: 38410715136.000000, content: 34306822144.000000, adv: 48.134426, pixel: 410389315584.000000]\n",
            "[Epoch 6/40] [Batch 4235/8720] [D loss: 0.000000] [G loss: 27551911936.000000, content: 24121901056.000000, adv: 48.609306, pixel: 343001169920.000000]\n",
            "[Epoch 6/40] [Batch 5236/8720] [D loss: 0.000000] [G loss: 11965771776.000000, content: 10795257856.000000, adv: 44.322350, pixel: 117051416576.000000]\n",
            "[Epoch 6/40] [Batch 6237/8720] [D loss: 0.000000] [G loss: 5795115008.000000, content: 4986822144.000000, adv: 44.912987, pixel: 80829292544.000000]\n",
            "[Epoch 6/40] [Batch 7238/8720] [D loss: 0.000000] [G loss: 3513169408.000000, content: 3006776832.000000, adv: 47.351448, pixel: 50639261696.000000]\n",
            "[Epoch 6/40] [Batch 8239/8720] [D loss: 0.000000] [G loss: 2548600832.000000, content: 2175282944.000000, adv: 42.536697, pixel: 37331775488.000000]\n",
            "[Epoch 7/40] [Batch 520/8720] [D loss: 0.000000] [G loss: 2431778560.000000, content: 2264301056.000000, adv: 49.613213, pixel: 16747740160.000000]\n",
            "[Epoch 7/40] [Batch 1521/8720] [D loss: 0.000000] [G loss: 1303371392.000000, content: 1202671872.000000, adv: 52.883091, pixel: 10069957632.000000]\n",
            "[Epoch 7/40] [Batch 2522/8720] [D loss: 0.000000] [G loss: 1012776304640.000000, content: 973713571840.000000, adv: 60.239777, pixel: 3906273542144.000000]\n",
            "[Epoch 7/40] [Batch 3523/8720] [D loss: 0.000000] [G loss: 264668119040.000000, content: 243592822784.000000, adv: 60.155872, pixel: 2107529756672.000000]\n",
            "[Epoch 7/40] [Batch 4524/8720] [D loss: 0.000000] [G loss: 226735341568.000000, content: 211246284800.000000, adv: 56.242676, pixel: 1548904955904.000000]\n",
            "[Epoch 7/40] [Batch 5525/8720] [D loss: 0.000000] [G loss: 79567142912.000000, content: 73447587840.000000, adv: 56.567661, pixel: 611955441664.000000]\n",
            "[Epoch 7/40] [Batch 6526/8720] [D loss: 0.000000] [G loss: 45471207424.000000, content: 41510522880.000000, adv: 57.663391, pixel: 396068421632.000000]\n",
            "[Epoch 7/40] [Batch 7527/8720] [D loss: 0.000000] [G loss: 27533314048.000000, content: 24428865536.000000, adv: 52.101032, pixel: 310444851200.000000]\n",
            "[Epoch 7/40] [Batch 8528/8720] [D loss: 0.000000] [G loss: 20380829696.000000, content: 18641555456.000000, adv: 69.436531, pixel: 173927464960.000000]\n",
            "[Epoch 8/40] [Batch 809/8720] [D loss: 0.000000] [G loss: 36448243712.000000, content: 34249177088.000000, adv: 61.796486, pixel: 219906621440.000000]\n",
            "[Epoch 8/40] [Batch 1810/8720] [D loss: 0.000000] [G loss: 13455056896.000000, content: 12673310720.000000, adv: 56.547821, pixel: 78174642176.000000]\n",
            "[Epoch 8/40] [Batch 2811/8720] [D loss: 0.000000] [G loss: 4516096000.000000, content: 4154034176.000000, adv: 60.697472, pixel: 36206178304.000000]\n",
            "[Epoch 8/40] [Batch 3812/8720] [D loss: 0.000000] [G loss: 15424138240.000000, content: 14580690944.000000, adv: 66.549240, pixel: 84344709120.000000]\n",
            "[Epoch 8/40] [Batch 4813/8720] [D loss: 0.000000] [G loss: 2302523392.000000, content: 2140764160.000000, adv: 62.406425, pixel: 16175923200.000000]\n",
            "[Epoch 8/40] [Batch 5814/8720] [D loss: 0.000000] [G loss: 4119729664.000000, content: 3757315584.000000, adv: 76.043655, pixel: 36241416192.000000]\n",
            "[Epoch 8/40] [Batch 6815/8720] [D loss: 0.000000] [G loss: 2033944448.000000, content: 1902569472.000000, adv: 75.221436, pixel: 13137499136.000000]\n",
            "[Epoch 8/40] [Batch 7816/8720] [D loss: 0.000000] [G loss: 1036056448.000000, content: 973740672.000000, adv: 80.570930, pixel: 6231575040.000000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EloW4JAcyYDJ",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MYIV5m89BlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNnCAsPZgyqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This function should take a degraded image in BGR format as a 250x250x3\n",
        "    numpy array with dtype np.uint8, and return its fixed version in the same format.\n",
        "\n",
        "    Incorrect formats will make the image look completely wrong.\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZk4RFZhnpab",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "Run this block after done to look at some of the results of the fix function yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akZdSIK8odKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "NUM_DISPLAY = 5\n",
        "\n",
        "files = glob('/content/rephrase-pubfig831/correct/test/*/*')\n",
        "grid = []\n",
        "\n",
        "for path in random.sample(files, NUM_DISPLAY):\n",
        "  correct = cv2.imread(path)\n",
        "  split = path.split('/')\n",
        "  degraded = cv2.imread('/'.join([*split[:3], 'degraded', *split[4:]]))\n",
        "  fixed = fix(degraded)\n",
        "  grid.append(np.column_stack([degraded, fixed, correct]))\n",
        "\n",
        "image = np.row_stack(grid)\n",
        "dpi = float(plt.rcParams['figure.dpi'])\n",
        "figsize = image.shape[1] / dpi, image.shape[0] / dpi\n",
        "ax = plt.figure(figsize=figsize).add_axes([0, 0, 1, 1])\n",
        "ax.axis('off')\n",
        "ax.imshow(image[..., ::-1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}